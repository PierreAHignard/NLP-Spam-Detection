{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891ec37d-282b-422b-9d69-40f78513f406",
   "metadata": {},
   "source": [
    "# Air Quality Data Analysis Workshop\n",
    "## Predicting PM2.5 Levels in African Cities\n",
    "\n",
    "**Learning Objectives:**\n",
    "By completing this workshop, you will be able to:\n",
    "- Handle time-series data with geographical groupings\n",
    "- Understand the challenges of missing data in time-series\n",
    "- Apply appropriate cross-validation strategies for grouped data\n",
    "- Perform feature engineering on temporal and geographical data\n",
    "- Build and evaluate regression models for environmental data\n",
    " \n",
    "**Context:**\n",
    "Air quality is a critical environmental and health issue. PM2.5 (particulate matter â‰¤ 2.5 micrometers) is particularly dangerous as these tiny particles can penetrate deep into lungs and bloodstream.\n",
    "\n",
    "This dataset contains air quality measurements from 4 African cities over time. Unlike typical machine learning problems, this data has **temporal dependencies** and **geographical groupings** \n",
    "that require special handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc516d6f-dc25-428f-8eb3-ae68e494d38d",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Data Loading and Initial Exploration\n",
    " \n",
    "Let's start by importing the necessary libraries and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c010286",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4e832",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configuration class to keep our paths and column names organized\n",
    "class Config:\n",
    "    path = \"../data/\"\n",
    "    id_col = \"id\"\n",
    "    target_col = \"pm2_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2004459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Load the training and test datasets\n",
    "# The training set contains our target variable (pm2_5) that we want to predict\n",
    "# The test set is what we'll make predictions on (without knowing the true pm2_5 values)\n",
    "\n",
    "train = pd.read_csv(Config.path + \"train.csv\")\n",
    "test = pd.read_csv(Config.path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909a90d-e6bc-4ab5-8b00-175d4321b0ee",
   "metadata": {},
   "source": [
    "**Exercise:** Explore the basic structure of our data.\n",
    "\n",
    "Use pandas methods to understand:\n",
    "- What columns do we have?\n",
    "- What are the data types?\n",
    "- How much data is in each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcefbf2-e5f8-4b85-82d9-f993b16bb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print the shape of these datasets\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb5f54-1661-47a0-9029-2f58a8f55281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display basic information about the training dataset\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display the first few rows to understand the data structure\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e611c-7516-42cd-b390-cf828990580f",
   "metadata": {},
   "source": [
    "**Exercise:** Explore the geographical and temporal scope.\n",
    "\n",
    "Find out:\n",
    "- Which countries and cities are in our dataset?\n",
    "- What is the time range of our data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fd663-166f-450a-8ec9-0bf4dbfeacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print unique countries and cities\n",
    "print(\"Unique countries:\", train['country'].unique())\n",
    "print(\"Unique cities:\", train['city'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467421f0-e679-4fbb-9b16-8788c01c0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print the time range of our data\n",
    "print(f\"Date range: {train['date'].min()} to {train['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1a976-2dac-45d0-b00f-37015dcda0d9",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "# 2. Data Visualization and Pattern Discovery\n",
    "  \n",
    "Before building models, we need to understand our data patterns. For air quality data, it's crucial to understand temporal trends and city-specific patterns. However, creating effective data visualizations requires more than just plotting data pointsâ€”it demands thoughtful design choices that follow established principles for clear communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d51596",
   "metadata": {},
   "source": [
    "**Exercise:** Configure global visualization styling for professional charts\n",
    "\n",
    "Effective data visualization requires consistent styling that eliminates visual clutter and ensures accessibility. As demonstrated in the air quality workshop, establishing global styling parameters creates a foundation for clean, professional charts throughout your analysis. This configuration applies principles of visual hierarchy and accessibility without requiring manual styling for each individual plot.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Seaborn style control](https://seaborn.pydata.org/generated/seaborn.set_style.html)\n",
    "- [Seaborn color palettes](https://seaborn.pydata.org/generated/seaborn.set_palette.html)\n",
    "- [Colorblind-friendly design](https://seaborn.pydata.org/tutorial/color_palettes.html#qualitative-color-palettes)\n",
    "\n",
    "Configure global styling settings by implementing a clean \"white\" background style that removes unnecessary visual elements while maintaining essential chart components. Remove grid lines and top/right spines to reduce clutter, but preserve left and bottom spines for data reference. Set slightly thicker axis lines for subtle emphasis. Additionally, apply a colorblind-friendly palette to ensure accessibility for users with color vision deficiency, making your visualizations inclusive for approximately 8% of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acefd0c-b8de-481e-80b6-1c7100b4406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Set global seaborn style - clean background without grid\n",
    "sns.set_style(\"white\", {\n",
    "    \"axes.grid\": False,           # Remove grid lines\n",
    "    \"axes.spines.left\": True,     # Keep left spine\n",
    "    \"axes.spines.bottom\": True,   # Keep bottom spine\n",
    "    \"axes.spines.top\": False,     # Remove top spine\n",
    "    \"axes.spines.right\": False,   # Remove right spine\n",
    "    \"axes.linewidth\": 1.2,        # Slightly thicker axis lines\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29153426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Set inclusive and colorblind-friendly palette\n",
    "# Using 'colorblind' palette which is accessible to colorblind users\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fdb67",
   "metadata": {},
   "source": [
    "**Exercise:** Select appropriate visualization for air quality data analysis\n",
    "\n",
    "Air quality data contains PM2.5 measurements taken at discrete time points, requiring careful visualization choices to avoid misrepresentation. Understanding which chart type best represents your data prevents misleading interpretations about pollution patterns. Consider how air quality monitoring works: measurements are individual observations at specific moments, not continuous streams of data.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Seaborn plot types overview](https://seaborn.pydata.org/tutorial/function_overview.html)\n",
    "- [Matplotlib plot gallery](https://matplotlib.org/stable/gallery/index.html)\n",
    "- [Choosing the right chart type](https://seaborn.pydata.org/tutorial/introduction.html#choosing-plot-types)\n",
    "\n",
    "Examine your air quality dataset and identify the most appropriate visualization for analyzing PM2.5 levels over time for a single city. Consider these options:\n",
    "- **Line plot**: Connects all measurement points with continuous lines\n",
    "- **Scatter plot**: Shows individual measurements as discrete points without connection\n",
    "- **Bar plot**: Shows each measurement as a separate categorical value\n",
    "- **Histogram**: Shows distribution of PM2.5 values across time periods\n",
    "\n",
    "Which visualization best represents the discrete nature of air quality measurements while still showing temporal patterns? Consider that PM2.5 readings are individual observations at specific times, not continuous processes, yet you want to identify underlying trends without implying false continuity between measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6545e4",
   "metadata": {},
   "source": [
    "**Exercise:** Implement professional air quality visualization\n",
    "\n",
    "Implement your selected visualization technique to display PM2.5 levels over time for Lagos city, ensuring the representation accurately reflects the measurement methodology. Your visualization should clearly show individual observations while maintaining clean, accessible design principles and scientific accuracy.\n",
    "\n",
    "**Documentation references:**\n",
    "- [plt.figure()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html) - Create figure and axes for visualization\n",
    "- [Matplotlib plotting functions](https://matplotlib.org/stable/api/pyplot_summary.html) - Various plotting methods available\n",
    "- [Seaborn statistical plotting](https://seaborn.pydata.org/api.html) - High-level statistical visualization functions\n",
    "- [Adding trend analysis](https://numpy.org/doc/stable/reference/routines.polynomials.polynomial.html) - Mathematical functions for pattern identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write a function to plot air quality data for a specific city\n",
    "\n",
    "def plot_air_quality_subplot(train_data, city_name, ax):\n",
    "    \"\"\"\n",
    "    Create air quality visualization for use in subplots\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: DataFrame containing training data with 'date', 'pm2_5', 'city' columns\n",
    "    - city_name: Name of the city to filter data for\n",
    "    - ax: Matplotlib Axes object to plot on\n",
    "\n",
    "    Returns:\n",
    "    - ax: The Axes object with the plot for further customization\n",
    "    \n",
    "    ADAPTATIONS FOR SUBPLOT:\n",
    "    - Receives ax parameter instead of creating plt.figure()\n",
    "    - Uses ax.scatter() and ax.plot() instead of plt.scatter() and plt.plot()\n",
    "    - Returns the ax object for further customization\n",
    "    - Removes plt.show() (handled by parent function)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Prepare data for the specified city\n",
    "    df_city = train_data[train_data['city'] == city_name].copy()\n",
    "    df_city = df_city.sort_values('date')\n",
    "    df_city['date_numeric'] = pd.to_datetime(df_city['date']).map(pd.Timestamp.toordinal)\n",
    "    \n",
    "    # Step 2: Create a plot for discrete measurements\n",
    "    ax.scatter(df_city['date'], df_city['pm2_5'], \n",
    "               alpha=0.6, \n",
    "               s=15,  # Smaller points for subplot\n",
    "               color=sns.color_palette(\"colorblind\")[0],\n",
    "               edgecolors='white',\n",
    "               linewidth=0.5,\n",
    "               label='Daily Measurements')\n",
    "    \n",
    "    # Step 3: Add polynomial trend line to show underlying pattern\n",
    "    z = np.polyfit(df_city['date_numeric'], df_city['pm2_5'], 2)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df_city['date'], p(df_city['date_numeric']), \n",
    "            color=sns.color_palette(\"colorblind\")[1],  \n",
    "            linewidth=1,\n",
    "            alpha=0.9,\n",
    "            linestyle='--',  # Dashed to distinguish from scatter\n",
    "            label='Trend Line')\n",
    "    \n",
    "    # Step 4: Add clear, descriptive title\n",
    "    ax.set_title(f\"PM2.5 Evolution in {city_name}\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Step 5: Label axes with units\n",
    "    ax.set_xlabel(\"Date\", fontsize=10)\n",
    "    ax.set_ylabel(\"PM2.5 (Âµg/mÂ³)\", fontsize=10)\n",
    "    \n",
    "    # Step 6: Improve date readability\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "    \n",
    "    # Step 7: Add legend (optional for subplots)\n",
    "    ax.legend(frameon=False, loc='upper right', fontsize=8)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a single subplot for Lagos\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "plot_air_quality_subplot(train, \"Lagos\", ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204abbee-c210-4bf6-b93b-20069fcda98a",
   "metadata": {},
   "source": [
    "**Exercise:** Create multi-city air quality comparison visualization\n",
    "\n",
    "Comparing air quality across multiple cities requires careful visual design to avoid overwhelming the audience. Using separate subplots for each city creates visual clutter and makes direct comparison difficult.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fcdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Get all unique cities from the dataset\n",
    "cities = train['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1cf8d3",
   "metadata": {},
   "source": [
    "To produce the figure :\n",
    "- Create 2x2 subplot structure for all cities\n",
    "- Plot each city using the subplot function\n",
    "- Add overall title for the entire figure\n",
    "- Optimize layout to prevent overlap and show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the whole figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, city in enumerate(cities):\n",
    "    plot_air_quality_subplot(train, city, axes[i])\n",
    "\n",
    "fig.suptitle(\"PM2.5 Air Quality with Trends Across African Cities\", \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)  # Make room for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a19dc5",
   "metadata": {},
   "source": [
    "**Exercise:** Improve subplot comparability\n",
    "\n",
    "Look at the current multi-city visualization. What problem do you observe when trying to compare PM2.5 levels between different cities in the current visualization?\n",
    "\n",
    "To make visual comparison possible, all subplots need the same y-axis limits.\n",
    "\n",
    "Modify the code above to:\n",
    "- Calculate the global minimum and maximum PM2.5 values across all cities before creating the subplots\n",
    "- Apply these fixed limits with `set_ylim` to each subplot's y-axis to ensure consistent scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57247114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create 2x2 subplot structure for all cities with the same max y value\n",
    "y_max = train['pm2_5'].max()\n",
    "\n",
    "cities = train['city'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel() \n",
    "\n",
    "\n",
    "for i, city in enumerate(cities):\n",
    "    axes[i].set_ylim(0, y_max)\n",
    "    plot_air_quality_subplot(train, city, axes[i])\n",
    "\n",
    "fig.suptitle(\"PM2.5 Air Quality with Trends Across African Cities\", \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065a65c-1fb8-4d0d-9b99-fcd7ee9a2bf2",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Data Cleaning and Missing Value Handling\n",
    " \n",
    "Real-world data often contains missing values that can significantly impact model performance. For time-series air quality data, missing values are particularly challenging because they can break temporal patterns and relationships. We need specialized approaches that preserve the sequential nature of our data while maintaining geographic consistency across different monitoring stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b7673-a0bb-4ef9-9169-33b15e95a79a",
   "metadata": {},
   "source": [
    "## 3.1 Missing Data Exploration\n",
    "\n",
    "Understanding missing data patterns is crucial before applying any cleaning strategy. Identifying which variables have missing data and their patterns helps us choose appropriate imputation strategies.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.isnull()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html)\n",
    "- [Missing data handling in pandas](https://pandas.pydata.org/docs/user_guide/missing_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ead0d-7e34-43fa-8463-6ed015a14af0",
   "metadata": {},
   "source": [
    "**Exercise:** Identify missing data patterns across variables\n",
    "- Which columns have missing values?\n",
    "- What percentage of data is missing in each column?\n",
    "  \n",
    "Generate a two-column DataFrame summarizing missing data patterns in the training dataset  with `Missing_Count` (absolute number of missing values) and `Missing_Percentage` (percentage of missing data per column). Sort the results by missing percentage in descending order and display only columns that contain at least one missing value to identify data quality issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c190a-d4db-4526-81ed-833f1a89ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Generate a summary of missing data in the training set\n",
    " \n",
    "missing_data = train.isnull().sum()\n",
    "missing_percentage = (missing_data / len(train) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for missing data\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"Missing data summary:\")\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dc25d-f477-4216-88e0-affe997775a7",
   "metadata": {},
   "source": [
    "**Exercise:** Remove columns with excessive missing data\n",
    " \n",
    "Variables with too much missing data provide little useful information. Removing them prevents our models from learning unreliable patterns based on sparse data.\n",
    "Remove columns with more than 70% missing data from both training and test datasets. Print the names of dropped columns and verify the new dataset dimensions to ensure data quality standards are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0430895",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TODO Remove columns with excessive missing data\n",
    "\n",
    "threshold = 0.7\n",
    "drop_cols = train.columns[train.isnull().sum() / len(train) > threshold]\n",
    "\n",
    "print(f\"Dropping columns with >{threshold*100}% missing data:\")\n",
    "print(list(drop_cols))\n",
    "\n",
    "train.drop(columns=drop_cols, inplace=True)\n",
    "test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "print(f\"New training data shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc098-fd9b-42a6-a572-0f8975a7e473",
   "metadata": {},
   "source": [
    "## 3.2 Forward-backward fill for time-series missing data\n",
    "\n",
    "Time-series data has temporal dependencies where adjacent time points are correlated. For air quality measurements, if PM2.5 is missing at 3 PM, the values at 2 PM and 4 PM are likely good estimates. Forward fill uses the last known value (2 PM â†’ 3 PM), while backward fill uses the next known value (4 PM â†’ 3 PM). Combining both methods ensures gaps at the beginning and end of time series are also filled, maximizing data retention while preserving temporal patterns.\n",
    "\n",
    "- **Forward fill (ffill)**: Use the last known value\n",
    "- **Backward fill (bfill)**: Use the next known value\n",
    "\n",
    "**IMPORTANT**: We must apply this **within each city** because each city has its own air quality patterns, measurement conditions, and temporal dynamics, so imputation should respect these geographic boundaries.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.ffill()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html)\n",
    "- [pandas.DataFrame.bfill()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html)\n",
    "- [pandas.DataFrame.groupby().transform()](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.transform.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ece918-a4bf-44cd-b495-327ef3f76f3b",
   "metadata": {},
   "source": [
    "**Exercise:** Build temporal imputation function for geographic groups\n",
    " \n",
    "Each city has unique pollution patterns that shouldn't be mixed. Creating a function ensures consistent data processing across the entire pipeline.\n",
    " \n",
    "Build a function that sorts data by group and date, then applies forward fill followed by backward fill within each group using pandas `.ffill()` and `.bfill()` methods. \n",
    "This ensures missing values are filled using temporally adjacent data points while respecting geographic boundaries (each city's data is processed separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b1ca6-29b3-4f9b-b94e-f722b6faab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a function to fill missing values using forward and backward fill\n",
    " \n",
    "def forward_back_fill(df, cols, group_col, date_col):\n",
    "    \"\"\"\n",
    "    Fill missing values using forward and backward fill within each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to process\n",
    "    - cols: List of columns to fill\n",
    "    - group_col: Column to group by (e.g., 'city')\n",
    "    - date_col: Date column for sorting\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with missing values filled\n",
    "    \"\"\"\n",
    "    # Sort by group and date to ensure proper temporal order\n",
    "    df = df.sort_values(by=[group_col, date_col])\n",
    "\n",
    "    # Apply forward fill then backward fill for each col within each group\n",
    "    for col in cols:\n",
    "        df[col] = df.groupby(group_col)[col].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40a005-b403-49c8-a8e9-26c1e365fd15",
   "metadata": {},
   "source": [
    "**Exercise:** Process missing data within each city independently for both training and test datasets.\n",
    "\n",
    "Loop through each city and for each dataset extract that city's data, identify columns with missing values, apply the forward-backward fill function, then update the original dataset. This ensures that each city's missing values are filled using only that city's own temporal patterns, maintaining geographic consistency across the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf7586-77dc-4472-8de0-4df11ded9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Process both datasets city by city\n",
    "\n",
    "for city in train['city'].unique():\n",
    "    # Process training data for this city\n",
    "    train_city = train[train['city'] == city].copy()\n",
    "    train_cols_missing = train_city.columns[train_city.isnull().any()]\n",
    "    if len(train_cols_missing) > 0:\n",
    "        train_city = forward_back_fill(train_city, train_cols_missing, \n",
    "                                     group_col=\"city\", date_col=\"date\")\n",
    "        train.update(train_city)\n",
    "    \n",
    "    # Process test data for this city\n",
    "    test_city = test[test['city'] == city].copy()\n",
    "    test_cols_missing = test_city.columns[test_city.isnull().any()]\n",
    "    if len(test_cols_missing) > 0:\n",
    "        test_city = forward_back_fill(test_city, test_cols_missing, \n",
    "                                    group_col=\"city\", date_col=\"date\")\n",
    "        test.update(test_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873e220-aa38-4da0-b124-b9f6f04f21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that missing values have been handled\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(f\"Train: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93363bef-8641-4319-a073-f78d31e41e6f",
   "metadata": {},
   "source": [
    "***\n",
    "# 4. Understanding Cross-Validation Strategy for Grouped Data\n",
    " \n",
    "Our data has an important characteristic: observations are grouped by city. Cities might have different climate patterns, pollution sources, or measurement conditions. Using random train/test splits could put data from the same city in both training and validation sets, leading to data leakage and overly optimistic performance estimates. We need a specialized cross-validation approach that respects these geographical groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087a1b2",
   "metadata": {},
   "source": [
    "## 4.1 The Problem with Standard Cross-Validation\n",
    "\n",
    "Standard k-fold cross-validation randomly splits data into folds, which can create data leakage when observations are naturally grouped. For geographical data like ours, this means a model could see data from Lagos in training and then be tested on different Lagos data points, artificially inflating performance metrics because cities have consistent internal patterns.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Cross-validation overview](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Data leakage in machine learning](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfcba8",
   "metadata": {},
   "source": [
    "**Exercise:** Understand why city grouping matters for validation\n",
    "\n",
    "Each city has unique pollution characteristics that create internal consistency. Random splitting would allow models to exploit these city-specific patterns rather than learning generalizable relationships.\n",
    "\n",
    "Examine the unique cities in your training data and consider how many data points each city contributes. Think about what would happen if training data included some Lagos measurements and validation data included other Lagos measurements - would this give us realistic performance estimates for predicting air quality in completely new cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print the number of measurements by city in our dataset\n",
    "\n",
    "city_distribution = train.groupby('city').size().reset_index(name='measurements')\n",
    "city_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11183c6b",
   "metadata": {},
   "source": [
    "## 4.2 GroupKFold Solution\n",
    "\n",
    "GroupKFold ensures that entire cities are either in training OR validation, never both. This provides more realistic performance estimates by testing the model's ability to generalize to completely unseen locations, which better reflects real-world deployment scenarios.\n",
    "\n",
    "**Documentation references:**\n",
    "- [GroupKFold documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html)\n",
    "- [Grouped data cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b313bb",
   "metadata": {},
   "source": [
    "**Exercise:** Create city-based cross-validation folds\n",
    "\n",
    "GroupKFold prevents data leakage by keeping all data from each city together. This tests our model's ability to generalize to new cities rather than memorizing city-specific patterns.\n",
    "Use GroupKFold with 4 splits to create folds where each fold contains complete cities. Set the groups parameter to the city column so that all measurements from the same city stay together. Store the fold assignments in a new 'folds' column for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76741b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create city-based folds using GroupKFold\n",
    "# Each fold will contain complete cities, not random samples\n",
    "\n",
    "gkf = GroupKFold(n_splits=4)\n",
    "train['folds'] = np.nan\n",
    "\n",
    "# The 'groups' parameter tells GroupKFold which samples belong to the same group (city)\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train, groups=train['city']), 1):\n",
    "    train.loc[val_idx, 'folds'] = fold\n",
    "\n",
    "# Convert to integer type for easier handling\n",
    "train['folds'] = train['folds'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc070fc-d5d0-467c-a153-4926c3dccb70",
   "metadata": {},
   "source": [
    "**Exercise:** Verify fold distribution and balance\n",
    "\n",
    "Understanding how cities are distributed across folds helps ensure our validation strategy is sound. Ideally, each fold should have reasonable representation and similar target distributions.\n",
    "Examine which cities are assigned to each fold and calculate basic statistics for the target variable (PM2.5) across folds. Check that folds are reasonably balanced in terms of sample size and target variable distribution to ensure robust cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eba9e4-7aeb-4d05-9b73-4d962c39b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check which cities are in each fold and how many samples per fold\n",
    "\n",
    "fold_distribution = train.groupby(['folds', 'city']).size().reset_index(name='count')\n",
    "\n",
    "print(\"Distribution of cities across folds:\")\n",
    "fold_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also check the target distribution across folds\n",
    "# We want to ensure folds are reasonably balanced\n",
    "\n",
    "fold_stats = train.groupby('folds')[Config.target_col].agg(['count', 'mean', 'std']).round(2)\n",
    "\n",
    "print(Config.target_col + \" statistics by fold:\")\n",
    "print(fold_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd547aa2-0c41-4eea-b422-8692ee74c0f2",
   "metadata": {},
   "source": [
    "***\n",
    "# 5. Feature Engineering for Time-Series and Geographic Data\n",
    " \n",
    "Feature engineering is crucial for time-series data. We need to extract meaningful patterns from dates and create geographic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b05f77-bc5b-4c2e-8ea8-742c404a8e8a",
   "metadata": {},
   "source": [
    "## 5.1 Temporal Feature Engineering\n",
    " \n",
    "Air quality data exhibits strong temporal patterns: pollution levels vary by season (winter heating, summer ozone), day of week (weekday traffic vs weekend), and time of day (rush hours). Extracting these cyclical patterns as features helps machine learning models capture these relationships explicitly rather than trying to infer them from raw timestamps.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Time series / date functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3eaca9-e6b5-4339-860e-d1ff3e9afd1f",
   "metadata": {},
   "source": [
    "**Exercise:** Understanding temporal patterns in air quality\n",
    " \n",
    "Before extracting features, we need to understand why temporal patterns matter for air quality prediction. \n",
    "Different times of year, days of week, and hours show different pollution patterns due to:\n",
    "- Seasonal effects: winter heating, summer photochemical reactions\n",
    "- Weekly cycles: higher traffic on weekdays vs weekends  \n",
    "- Daily patterns: rush hour peaks, industrial activity schedules\n",
    " \n",
    "Create the `extract_temporal_features` function that extracts these meaningful time components from datetime data to help our model understand when pollution is typically higher or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46995e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a function to extract temporal features from a datetime column\n",
    "\n",
    "def extract_temporal_features(df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Extract temporal features from a datetime column to capture cyclical patterns.\n",
    "    \n",
    "    Air quality varies with seasonal changes, weekly patterns (weekdays vs weekends),\n",
    "    and daily cycles. These temporal features help models understand when pollution\n",
    "    levels are typically higher or lower.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date column\n",
    "    - date_col: Name of the datetime column to extract features from\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with additional temporal features (year, month, day, quarter, week and dayofweek)\n",
    "    \"\"\"\n",
    "    # Ensure the date column is datetime type for proper extraction\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Extract basic temporal components that affect air quality\n",
    "    df['year'] = df[date_col].dt.year           # Long-term trends\n",
    "    df['month'] = df[date_col].dt.month         # Seasonal patterns (heating season, etc.)\n",
    "    df['day'] = df[date_col].dt.day             # Monthly patterns\n",
    "    df['quarter'] = df[date_col].dt.quarter     # Quarterly business cycles\n",
    "    df['week'] = df[date_col].dt.isocalendar().week.astype(\"int64\")  # Weekly patterns\n",
    "    df['dayofweek'] = df[date_col].dt.dayofweek # 0=Monday, 6=Sunday (weekday vs weekend)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdf50c-13b2-46ad-825f-52bedde4158c",
   "metadata": {},
   "source": [
    "**Exercise:** Apply temporal feature engineering to both datasets\n",
    "\n",
    "Use your function to extract temporal features from both training and test datasets. This ensures both datasets have the same feature structure for consistent model training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788ba35-4788-427f-9c1a-2578bafd09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply temporal feature to train & test sets\n",
    "\n",
    "train = extract_temporal_features(train)\n",
    "test = extract_temporal_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08aa7d-c3ae-4eab-8191-44715c5d0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new temporal features\n",
    "\n",
    "print(\"New temporal features created:\")\n",
    "temporal_cols = ['year', 'month', 'day', 'quarter', 'week', 'dayofweek']\n",
    "print(train[['date'] + temporal_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f20c9-26e7-45f8-a429-9885ce1bb01c",
   "metadata": {},
   "source": [
    "## 5.2 Geographic Feature Engineering\n",
    " \n",
    "Air quality monitoring stations at different geographic locations can have distinct pollution profiles due to local factors like traffic density, industrial proximity, topography, and meteorological conditions. Creating location-specific identifiers helps models learn these location-based patterns.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.astype()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "- [String concatenation in pandas](https://pandas.pydata.org/docs/user_guide/text.html#concatenation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b257b3-a2a6-4a4e-a2e8-282ddfd67b52",
   "metadata": {},
   "source": [
    "**Exercise 1:** Understanding the importance of geographic features\n",
    " \n",
    "Different monitoring stations can have vastly different pollution levels even within the same city due to local factors like proximity to highways, industrial zones, or natural barriers.\n",
    "By creating unique location identifiers, we help the model distinguish between these micro-environments and learn location-specific pollution patterns.\n",
    "\n",
    "Implement the `extract_geographic_features` function that combines latitude and longitude coordinates to create unique `location` identifier (`latitude_longitude` format) for each monitoring station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a function to extract geographic features from latitude and longitude coordinates\n",
    " \n",
    "def extract_geographic_features(df):\n",
    "    \"\"\"\n",
    "    Create geographic features from latitude and longitude coordinates.\n",
    "    \n",
    "    Different monitoring locations have unique pollution characteristics based on\n",
    "    their surroundings (urban vs rural, near highways, industrial areas, etc.).\n",
    "    Combining coordinates creates unique location identifiers that help models\n",
    "    learn location-specific pollution patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with site_latitude and site_longitude columns\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with additional geographic feature (location identifier)\n",
    "    \"\"\"\n",
    "    # Create a unique location identifier by combining coordinates\n",
    "    # This allows the model to learn location-specific patterns\n",
    "    df['location'] = df['site_latitude'].astype('str') + '_' + df['site_longitude'].astype('str')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f750a-db6a-417c-a389-362a64439363",
   "metadata": {},
   "source": [
    "**Exercise 2:** Apply geographic feature engineering\n",
    " \n",
    "Use your function to create location identifiers for both training and test datasets.\n",
    "This will help the model understand that different coordinate pairs represent different pollution environments with potentially different baseline levels and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552ae0a-ba80-463a-a85a-f36827a5ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply geographic feature to train & test sets\n",
    "\n",
    "train = extract_geographic_features(train)\n",
    "test = extract_geographic_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504accd-e243-4848-bba5-3bdce5252019",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created {train['location'].nunique()} unique locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e43246-4283-4264-a752-442a99c7c4dd",
   "metadata": {},
   "source": [
    "## 5.3 Categorical Feature Encoding\n",
    " \n",
    "Machine learning algorithms require numerical inputs, but our dataset contains categorical variables like location identifiers and dates. We need to convert these text-based categories into numerical representations while ensuring consistency between training and test datasets. **Label Encoding** assigns a unique integer to each category. \n",
    "\n",
    "**Documentation references:**\n",
    "- [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68576783-5750-41d5-8f6b-0666c6ee259c",
   "metadata": {},
   "source": [
    "**Exercise 1:** Understanding the need for categorical encoding\n",
    " \n",
    "Machine learning models like Linear Regression work with numbers, not text. When we have categorical variables like location names or dates, we need to convert them to numerical format. However, we must ensure that the same category gets the same number in both training and test datasets to maintain consistency.\n",
    "\n",
    "Implement the `encode_categorical_features` function that uses Label Encoding to convert categorical variables to integers while maintaining consistency between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write a function to encode categorical features\n",
    "def encode_categorical_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Encode categorical features for machine learning models.\n",
    "    \n",
    "    Most ML algorithms require numerical inputs, but our data contains categorical\n",
    "    variables (location names, dates) that need conversion to numbers. We use \n",
    "    Label Encoding to assign unique integers to each category while ensuring\n",
    "    consistent encoding between training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df, test_df: Training and test DataFrames with categorical features\n",
    "\n",
    "    Returns:\n",
    "    - train_encoded: Encoded training DataFrame\n",
    "    - test_encoded: Encoded test DataFrame  \n",
    "    - feature_cols: List of column names to use as model features\n",
    "    \"\"\"\n",
    "    # Combine datasets to ensure consistent encoding across train/test\n",
    "    # This prevents issues where test set has categories not seen in training\n",
    "    combined_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Initialize label encoder for consistent categorical-to-numerical conversion\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Identify categorical columns that need encoding\n",
    "    # These are high-cardinality categories (many unique values)\n",
    "    categorical_cols = ['location', 'date']\n",
    "\n",
    "    # Apply label encoding: convert each unique category to a unique integer\n",
    "    for col in categorical_cols:\n",
    "        if col in combined_data.columns:\n",
    "            combined_data[col] = le.fit_transform(combined_data[col])\n",
    "\n",
    "    # Split back into separate train and test datasets\n",
    "    train_encoded = combined_data[combined_data['id'].isin(train_df['id'])]\n",
    "    test_encoded = combined_data[combined_data['id'].isin(test_df['id'])]\n",
    "\n",
    "    # Define feature columns (exclude target variable and metadata)\n",
    "    exclude_cols = [Config.target_col, Config.id_col, 'folds', 'country', 'city', \n",
    "                   'site_id', 'site_latitude', 'site_longitude']\n",
    "    feature_cols = [col for col in train_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "    return train_encoded, test_encoded, feature_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6576897-a8ce-4821-ae55-aeb9054febfa",
   "metadata": {},
   "source": [
    "**Exercise 2:** Apply categorical encoding to prepare data for modeling\n",
    "\n",
    "Use your function to encode categorical features in both datasets and identify which columns should be used as features for model training (excluding target variables and metadata that shouldn't influence predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40340a1f-665f-4a95-bfdb-c8f38f4fb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply the function to encode categorical features train and test sets\n",
    "\n",
    "train_processed, test_processed, feature_columns = encode_categorical_features(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2016623-131b-4783-83e3-3f7d4f0a577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total feature columns for modeling: {len(feature_columns)}\")\n",
    "print(f\"Feature columns: {feature_columns}...\")\n",
    "\n",
    "# Verify encoding worked correctly\n",
    "print(f\"\\nData shapes after encoding:\")\n",
    "print(f\"Training data: {train_processed.shape}\")\n",
    "print(f\"Test data: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0463b7f-6615-4449-9413-1da1a498635e",
   "metadata": {},
   "source": [
    "***\n",
    "# 6. Baseline Model Development\n",
    " \n",
    "Building a baseline model is essential before exploring complex algorithms. A simple Linear Regression model provides a reference point for measuring improvement and helps validate that our data preprocessing and cross-validation strategy work correctly. Starting with interpretable models also helps us understand which features are most important for air quality prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bc130",
   "metadata": {},
   "source": [
    "## 6.1 Train-Validation Split Using Fold Strategy\n",
    "\n",
    "We'll use our city-based folds to create a proper train-validation split that respects geographical boundaries. This ensures our baseline performance estimates are realistic and comparable to our final cross-validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372dcf7a-0654-4239-be5b-9dac4f0cbf59",
   "metadata": {},
   "source": [
    "**Exercise:** Create geographically-separated train and validation sets\n",
    "\n",
    "Using fold-based splitting ensures our model evaluation reflects real-world deployment conditions. We need distinct city groups for training and validation to avoid data leakage.\n",
    "\n",
    "\n",
    "```\n",
    "ğŸ“Š FOLD-BASED DATA SPLITTING STRATEGY\n",
    "\n",
    "Original Dataset (train_processed)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  FOLD 1    â”‚  FOLD 2    â”‚  FOLD 3    â”‚  FOLD 4    â”‚\n",
    "â”‚  City A    â”‚  City B    â”‚  City C    â”‚  City D    â”‚\n",
    "â”‚  [data]    â”‚  [data]    â”‚  [data]    â”‚  [data]    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                    â†“ SPLIT â†“\n",
    "\n",
    "TRAINING DATA (Folds 1, 3, 4)          VALIDATION DATA (Fold 2)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  City A + City C + City D       â”‚    â”‚      City B         â”‚\n",
    "â”‚                                 â”‚    â”‚                     â”‚\n",
    "â”‚  â†“ SEPARATE FEATURES & TARGET   â”‚    â”‚  â†“ SEPARATE FEATURESâ”‚\n",
    "â”‚                                 â”‚    â”‚    & TARGET         â”‚\n",
    "â”‚  X_train: [features matrix]     â”‚    â”‚  X_val: [features]  â”‚\n",
    "â”‚  y_train: [PM2.5 values]        â”‚    â”‚  y_val: [PM2.5]     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ¯ RESULT: Complete geographical separation\n",
    "   â†’ No city appears in both training and validation\n",
    "   â†’ Prevents data leakage and ensures realistic evaluation\n",
    "```\n",
    "\n",
    "\n",
    "Use folds 1, 3, 4 for training and fold 2 for validation to create train_data and val_data subsets. Then prepare feature matrices `(X_train, X_val)` and target vectors `(y_train, y_val)` using the feature_columns list from our preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da32b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Select the relevant folds for train and val data\n",
    "training_folds = [1, 3, 4]\n",
    "validation_fold = [2]\n",
    "\n",
    "train_data = train_processed[train_processed['folds'].isin(training_folds)]\n",
    "val_data = train_processed[train_processed['folds'].isin(validation_fold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Prepare feature matrices X_train, y_train and target vectors X_val, y_val\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[Config.target_col]\n",
    "X_val = val_data[feature_columns]\n",
    "y_val = val_data[Config.target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b4167",
   "metadata": {},
   "source": [
    "## 6.2 Linear Regression Model Training and Evaluation\n",
    "\n",
    "Linear Regression assumes linear relationships between features and target variables. While this assumption may not hold perfectly for air quality data, it provides an interpretable baseline and helps identify the most predictive features through coefficient analysis.\n",
    "\n",
    "Documentation references:\n",
    "- [Linear Regression in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [Mean Squared Error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)\n",
    "- [RÂ² score](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e54ce5-ea53-4ba5-b667-625c27018fac",
   "metadata": {},
   "source": [
    "**Exercise:** Train baseline Linear Regression model\n",
    "\n",
    "Establishing baseline performance helps gauge whether more complex models provide meaningful improvements. Simple models also help identify obvious data quality issues early in the pipeline.\n",
    "\n",
    "Train a LinearRegression model on the training data, make predictions on the validation set, and calculate three key performance metrics: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and RÂ² score to establish our baseline performance benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train the model\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc275a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Make predictions\n",
    "\n",
    "y_pred = baseline_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740c736",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze baseline model performance metrics\n",
    "\n",
    "Understanding baseline performance provides context for evaluating more sophisticated approaches. These metrics help determine if our preprocessing pipeline is working correctly and set expectations for model improvement.\n",
    "\n",
    "Display the calculated performance metrics (MSE, RMSE, RÂ²) with clear labels and appropriate decimal precision. Interpret what these numbers mean in the context of air quality prediction - RMSE shows average prediction error in Âµg/mÂ³, while RÂ² indicates the proportion of variance explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce420c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Calculate performance metrics\n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = root_mean_squared_error(y_val, y_pred)\n",
    "r2_score = baseline_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b0d78-5321-4bd4-a147-6e81b736988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8d683-96f3-45df-9165-7ec15b4683dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Feature Selection for Model Improvement\n",
    " \n",
    "Not all features are equally important for predicting air quality. Feature selection can improve model performance by focusing on the most relevant variables, reduce overfitting by eliminating noise, and enhance interpretability by identifying the key factors that drive PM2.5 levels. We'll explore two complementary approaches: statistical selection and recursive elimination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406a480-52b5-4ddc-b461-d472f8275947",
   "metadata": {},
   "source": [
    "## 7.1 SelectKBest Feature Selection\n",
    "\n",
    "SelectKBest uses statistical tests to identify features with the strongest linear relationships to the target variable. For regression problems, it employs F-statistics to measure how much each feature explains the variance in PM2.5 levels. This method is fast and effective for identifying obviously important features, making it ideal for initial feature screening.\n",
    "\n",
    "**Documentation references:**\n",
    "- [SelectKBest documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "- [f_regression scoring function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "- [Feature selection guide](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e2ff2-89dc-4cfa-9c6b-722c73e6f97b",
   "metadata": {},
   "source": [
    "**Exercise:** Apply statistical feature selection with `SelectKBest`\n",
    "\n",
    "Statistical tests quickly identify features with strong predictive power. This approach helps eliminate obviously irrelevant variables before applying more sophisticated selection methods.\n",
    "\n",
    "Initialize `SelectKBest` with `f_regression` scoring and `k=15` features. Fit the selector on training data, transform both training and validation sets, then create a summary DataFrame showing feature names, their F-scores, and selection status to understand which variables are most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 15 features\n",
    "\n",
    "k_best_features = 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Initialize and fit the selector\n",
    "\n",
    "selector_kbest = SelectKBest(score_func=f_regression, k=k_best_features)\n",
    "X_train_kbest = selector_kbest.fit_transform(X_train, y_train)\n",
    "X_val_kbest = selector_kbest.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Get selected feature names and their scores\n",
    "\n",
    "selected_features_kbest = np.array(feature_columns)[selector_kbest.get_support()]\n",
    "feature_scores = selector_kbest.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a summary DataFrame\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Score': feature_scores,\n",
    "    'Selected': selector_kbest.get_support()\n",
    "}).sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 features by SelectKBest score:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18f685-9ac8-450d-9b7b-e6ab7665b981",
   "metadata": {},
   "source": [
    "**Exercise:** Evaluate `SelectKBest` model performance against baseline\n",
    "\n",
    "Comparing feature selection results to the baseline helps determine if reducing dimensionality improves generalization. Sometimes fewer, more relevant features outperform using all available variables.\n",
    "\n",
    "Train a new LinearRegression model using only the `SelectKBest` features, make predictions on the validation set, and calculate `MSE`, `RMSE` and `RÂ²` metrics. Compare these results to the baseline performance to assess whether feature selection improved model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a19762-0f39-4db6-b648-b96ec3b1154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train a new model using only the selected features\n",
    " \n",
    "kbest_model = LinearRegression()\n",
    "kbest_model.fit(X_train_kbest, y_train)\n",
    "\n",
    "y_pred_kbest = kbest_model.predict(X_val_kbest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03db847-f9dc-43e9-939d-ff32547e7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compute performance metrics for the SelectKBest model\n",
    "\n",
    "mse_kbest = mean_squared_error(y_val, y_pred_kbest)\n",
    "rmse_kbest = root_mean_squared_error(y_val, y_pred_kbest)\n",
    "r2_kbest = kbest_model.score(X_val_kbest, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SelectKBest Model Performance:\")\n",
    "print(f\"MSE: {mse_kbest:.2f} (baseline: {mse:.2f})\")\n",
    "print(f\"RMSE: {rmse_kbest:.2f} (baseline: {rmse:.2f})\")\n",
    "print(f\"RÂ² Score: {r2_kbest:.4f} (baseline: {r2_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67f813-8832-41d5-ad1a-d45ff53f3e3c",
   "metadata": {},
   "source": [
    "## 7.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE takes a different approach by iteratively training models and eliminating the least important features based on model coefficients. This method considers feature interactions and dependencies that univariate tests might miss, potentially finding more nuanced feature combinations that work well together for air quality prediction.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Recursive feature elimination guide](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination)\n",
    "- [RFE documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34688f68-0d2a-499b-a8bd-efd3020dc3f1",
   "metadata": {},
   "source": [
    "**Exercise:** Apply model-based feature selection with RFE\n",
    "\n",
    "`RFE` considers how features work together within the model context. Unlike statistical tests, it accounts for feature interactions and multicollinearity when making selection decisions.\n",
    "\n",
    "Initialize `RFE` with `LinearRegression` as the estimator and select `15` features. Fit `RFE` on training data, transform both datasets, then examine the selected features and their model coefficients to understand which variables the model considers most important in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc22763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 15 features\n",
    "rfe_features = 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Initialize RFE with LinearRegression as the estimator\n",
    "estimator = LinearRegression()\n",
    "selector_rfe = RFE(estimator=estimator, n_features_to_select=rfe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Fit RFE and transform features\n",
    "X_train_rfe = selector_rfe.fit_transform(X_train, y_train)\n",
    "X_val_rfe = selector_rfe.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c951768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Get selected features and their rankings\n",
    "selected_features_rfe = np.array(feature_columns)[selector_rfe.support_]\n",
    "feature_rankings = selector_rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab960e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features selected by RFE:\")\n",
    "rfe_df = pd.DataFrame({\n",
    "    'Feature': selected_features_rfe,\n",
    "    'Coefficient': selector_rfe.estimator_.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(rfe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fbfde-6741-40fd-9bae-ab3963e6bb93",
   "metadata": {},
   "source": [
    "**Exercise:** Compare RFE performance with previous approaches\n",
    "\n",
    "Model-based selection might identify different feature combinations than statistical tests. Comparing both approaches helps determine which selection strategy works better for our specific air quality prediction task.\n",
    "\n",
    "Train a `LinearRegression` model using RFE-selected features, calculate performance metrics (`MSE`, `RMSE`, `RÂ²`), and compare results with both baseline and SelectKBest approaches to identify the most effective feature selection method for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812779ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train a new model using only the selected features\n",
    "\n",
    "rfe_model = LinearRegression()\n",
    "rfe_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "y_pred_rfe = rfe_model.predict(X_val_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4c324-1e74-4f66-a4f0-383d1d61db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compute performance metrics for the RFE model\n",
    "\n",
    "mse_rfe = mean_squared_error(y_val, y_pred_rfe)\n",
    "rmse_rfe = root_mean_squared_error(y_val, y_pred_rfe)\n",
    "r2_rfe = rfe_model.score(X_val_rfe, y_val)\n",
    "\n",
    "# Display RFE model performance\n",
    "print(\"RFE Model Performance:\")\n",
    "print(f\"RMSE: {rmse_rfe:.2f}\")\n",
    "print(f\"RÂ² Score: {r2_rfe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af3977-e80c-457e-adc5-909ec80aa400",
   "metadata": {},
   "source": [
    "***\n",
    "# 8. Cross-Validation Evaluation\n",
    " \n",
    "Single train-validation splits can give misleading performance estimates, especially with geographical data where one city might be easier or harder to predict than others. Cross-validation provides more robust and reliable performance estimates by testing our model across all city combinations. This comprehensive evaluation helps us understand model consistency and identify potential overfitting to specific geographical patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08841f0",
   "metadata": {},
   "source": [
    "## 8.1 Implementing K-Fold Cross-Validation with Best Features\n",
    "\n",
    "We'll evaluate our best-performing feature selection method across all four folds to get a comprehensive view of model performance. This approach ensures our performance estimates aren't biased by the particular choice of validation cities and provides confidence intervals for our metrics.\n",
    "\n",
    "Documentation references:\n",
    "- [Cross-validation in scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Model evaluation best practices](https://scikit-learn.org/stable/modules/model_evaluation.html#cross-validation)\n",
    "- [Performance metric interpretation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ccd89-e42a-4f41-a848-817013800c03",
   "metadata": {},
   "source": [
    "**Exercise:** Select optimal feature set from previous experiments\n",
    "\n",
    "Choosing the best-performing feature selection method ensures our cross-validation uses the most effective variable combination. This decision impacts all subsequent model evaluations and comparisons.\n",
    "\n",
    "Compare the `RMSE` results from `SelectKBest` and `RFE` approaches from the previous section. Choose the method that achieved lower validation RMSE and assign its selected features to a variable called `selected_features` for use in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best performing feature selection method\n",
    "selected_features = selected_features_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834eb54",
   "metadata": {},
   "source": [
    "**Exercise:** Implement comprehensive cross-validation across all folds\n",
    "\n",
    "Testing across all fold combinations provides robust performance estimates. Each city serves as validation data exactly once, ensuring fair evaluation across different geographical contexts.\n",
    "\n",
    "Create a loop that iterates through folds 1-4, using each fold as validation while training on the remaining three folds. For each iteration, prepare the feature matrices using `selected_features`, train a `LinearRegression` model, make predictions, and calculate `RMSE` and `RÂ²` scores to build a comprehensive performance profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd404334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement comprehensive cross-validation across all folds\n",
    "\n",
    "# Initialize lists to store results\n",
    "fold_rmse_scores = []\n",
    "fold_r2_scores = []\n",
    "\n",
    "print(\"\\nCross-validation results:\")\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    # Create train/validation split for current fold\n",
    "    train_fold = train_processed[train_processed['folds'] != fold]\n",
    "    val_fold = train_processed[train_processed['folds'] == fold]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train_cv = train_fold[selected_features]\n",
    "    y_train_cv = train_fold[Config.target_col]\n",
    "    X_val_cv = val_fold[selected_features]\n",
    "    y_val_cv = val_fold[Config.target_col]\n",
    "    \n",
    "    # Train model\n",
    "    cv_model = LinearRegression()\n",
    "    cv_model.fit(X_train_cv, y_train_cv)\n",
    "    \n",
    "    # Make predictions and evaluate\n",
    "    y_pred_cv = cv_model.predict(X_val_cv)\n",
    "    \n",
    "    fold_rmse = root_mean_squared_error(y_val_cv, y_pred_cv)\n",
    "    fold_r2 = cv_model.score(X_val_cv, y_val_cv)\n",
    "    \n",
    "    fold_rmse_scores.append(fold_rmse)\n",
    "    fold_r2_scores.append(fold_r2)\n",
    "    \n",
    "    print(f\"Fold {fold}: RMSE = {fold_rmse:.2f}, RÂ² = {fold_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb409201",
   "metadata": {},
   "source": [
    "## 8.2 Statistical Analysis of Cross-Validation Results\n",
    "\n",
    "Analyzing the distribution of performance across folds helps us understand model stability and reliability. High variance between folds might indicate overfitting or geographical bias, while consistent performance suggests robust generalization capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bec05",
   "metadata": {},
   "source": [
    "**Exercise:** Calculate aggregate performance statistics with confidence measures\n",
    "\n",
    "Understanding performance variability across folds helps assess model reliability. Standard deviations indicate how consistent the model performs across different geographical contexts.\n",
    "\n",
    "Calculate the `mean` and `standard deviation` of `RMSE` and `RÂ²` scores across all folds. Display the results in the format `mean Â± std` to show both central tendency and variability, providing a complete picture of model performance consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23152371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Calculate and print overall performance\n",
    "\n",
    "mean_rmse = np.mean(fold_rmse_scores)\n",
    "std_rmse = np.std(fold_rmse_scores)\n",
    "mean_r2 = np.mean(fold_r2_scores)\n",
    "std_r2 = np.std(fold_r2_scores)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE: {mean_rmse:.2f} Â± {std_rmse:.2f}\")\n",
    "print(f\"Average RÂ²: {mean_r2:.4f} Â± {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75965b59-56f4-4924-a375-3f0387243cde",
   "metadata": {},
   "source": [
    "***\n",
    "# 9. Results Analysis and Interpretation\n",
    "\n",
    "Understanding model performance goes beyond simple metrics. We need to visualize prediction quality, interpret feature importance, and contextualize our results within real-world air quality standards. This analysis helps determine if our model is ready for practical deployment and identifies areas for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af813352",
   "metadata": {},
   "source": [
    "## 9.1 Model Performance Visualization\n",
    "\n",
    "Visual analysis reveals patterns that summary statistics might miss. Scatter plots of actual vs predicted values show prediction accuracy across different pollution levels, while residual plots help identify systematic errors or heteroscedasticity that could indicate model limitations.\n",
    "\n",
    "Documentation references:\n",
    "- [Residuals vs. Fits Plot](https://online.stat.psu.edu/stat462/node/117/)\n",
    "- [Residual Plot: Definition and Examples](https://www.statisticshowto.com/residual-plot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69369904-8637-4526-80bf-54d6807da86b",
   "metadata": {},
   "source": [
    "**Exercise:** Create diagnostic plots for prediction quality assessment\n",
    "\n",
    "Visual diagnostics reveal model strengths and weaknesses across different prediction ranges. Good models show tight clustering around the diagonal line and random residual patterns.\n",
    "\n",
    "Create two diagnostic subplots to evaluate prediction quality:\n",
    "- **Actual vs Predicted scatter plot**: Points close to the diagonal line (y=x) indicate accurate predictions, while points far from the line show prediction errors\n",
    "- **Residuals plot**: Calculate residuals as (actual - predicted) values, then plot them against predicted values - random scatter around zero indicates good model fit, while patterns suggest systematic errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1061b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a comparison plot of actual vs predicted values\n",
    "\n",
    "# Use the last fold's predictions for visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val_cv, y_pred_cv, alpha=0.6, s=20)\n",
    "plt.plot([y_val_cv.min(), y_val_cv.max()], [y_val_cv.min(), y_val_cv.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual PM2.5')\n",
    "plt.ylabel('Predicted PM2.5')\n",
    "plt.title('Actual vs Predicted PM2.5')\n",
    "\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_val_cv - y_pred_cv\n",
    "plt.scatter(y_pred_cv, residuals, alpha=0.6, s=20)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted PM2.5')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfb773",
   "metadata": {},
   "source": [
    "## 9.2 Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive predictions helps validate model logic and provides insights for air quality management. Important features should align with domain knowledge about pollution sources and meteorological influences on air quality.\n",
    "\n",
    "Documentation references:\n",
    "- [Feature importance interpretation](https://scikit-learn.org/stable/modules/feature_selection.html#feature-importance-scores)\n",
    "- [Linear regression coefficients](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcba57-5688-44ea-a0b4-b47ec6796a64",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize most influential features for model predictions\n",
    "\n",
    "Feature importance analysis validates model logic and provides actionable insights. Important features should make scientific sense for air quality prediction.\n",
    "\n",
    "Create a horizontal bar chart showing the top 10 most important features. Use the method that performed better. Sort features by importance and add appropriate labels to help interpret which variables most strongly influence PM2.5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a visualization of the most important features\n",
    "\n",
    "if rmse_kbest < rmse_rfe:\n",
    "    # Use SelectKBest scores\n",
    "    importance_data = feature_importance_df[feature_importance_df['Selected']].head(10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance_data)), importance_data['Score'])\n",
    "    plt.yticks(range(len(importance_data)), importance_data['Feature'])\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.title('Top 10 Features (SelectKBest)')\n",
    "else:\n",
    "    # Use RFE coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(rfe_df)), abs(rfe_df['Coefficient']))\n",
    "    plt.yticks(range(len(rfe_df)), rfe_df['Feature'])\n",
    "    plt.xlabel('Absolute Coefficient Value')\n",
    "    plt.title('Feature Importance (RFE)')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04551a07",
   "metadata": {},
   "source": [
    "## 9.3 Contextual Performance Interpretation\n",
    "\n",
    "Raw performance metrics need context to be meaningful. Comparing our prediction errors to WHO air quality guidelines and typical PM2.5 ranges helps assess whether our model accuracy is sufficient for practical applications like public health warnings or policy decisions.\n",
    "\n",
    "Documentation references:\n",
    "- [WHO Air Quality Guidelines](https://iris.who.int/bitstream/handle/10665/345329/9789240034228-eng.pdf)\n",
    "- [PM2.5 health impact thresholds](https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf) (page 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5213b9",
   "metadata": {},
   "source": [
    "**Exercise:** Interpret model performance within air quality context\n",
    "\n",
    "Understanding prediction accuracy relative to health thresholds helps determine model utility. Prediction errors should be small compared to differences between air quality categories.\n",
    "\n",
    "Create a comprehensive summary showing final RMSE and RÂ² with their interpretations. Include WHO air quality guidelines for PM2.5 levels and explain how our prediction accuracy relates to these public health thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163617f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO Create summary statistics\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Final RMSE: {mean_rmse:.2f} Âµg/mÂ³\")\n",
    "print(f\"Final RÂ² Score: {mean_r2:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"- On average, our predictions are off by {mean_rmse:.1f} Âµg/mÂ³\")\n",
    "print(f\"- The model explains {mean_r2*100:.1f}% of the variance in PM2.5 levels\")\n",
    "print(f\"- Standard deviation of RMSE across folds: {std_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f983615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Find in documents some context information for PM2.5 levels (WHO guidelines)\n",
    "\n",
    "print()\n",
    "print(\"WHO Air Quality Guidelines (PM2.5):\")\n",
    "print(\"- Good: 0-12 Âµg/mÂ³\")\n",
    "print(\"- Moderate: 12-35 Âµg/mÂ³\") \n",
    "print(\"- Unhealthy: 35-55 Âµg/mÂ³\")\n",
    "print(\"- Very Unhealthy: 55+ Âµg/mÂ³\")\n",
    "\n",
    "# Make a conclusion based on the model performance and WHO guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6454bed",
   "metadata": {},
   "source": [
    "With prediction errors of 27.1 Âµg/mÂ³, our model cannot reliably distinguish between WHO air quality categories, as this error spans multiple health risk levels (e.g., 'Good' to 'Unhealthy' range)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Air Quality (uv)",
   "language": "python",
   "name": "air_quality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
