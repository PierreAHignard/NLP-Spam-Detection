{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9200af3d-175d-4a69-8d28-04ed3787b347",
   "metadata": {},
   "source": [
    "# Spam Detection Workshop\n",
    "## Text Classification for SMS and Email Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03076548",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "By completing this workshop, you will be able to:\n",
    "- Understand fundamental concepts of Natural Language Processing (NLP)\n",
    "- Handle text preprocessing and tokenization strategies\n",
    "- Apply feature extraction techniques for text classification\n",
    "- Implement and evaluate machine learning models for text data\n",
    "- Compare model performance across different datasets and scenarios\n",
    "- Understand the challenges of domain transfer in text classification\n",
    "\n",
    "**Context:**\n",
    "Spam detection is a critical application of text classification that helps protect users from unwanted messages. This workshop uses two different datasets (SMS and email messages) to explore how machine learning models perform across different text domains and communication channels.\n",
    "\n",
    "Unlike numerical data, text requires special preprocessing steps including tokenization, feature extraction, and encoding before machine learning algorithms can process it effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794560d3-3370-4f92-bde4-bd562426d0c6",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Library Setup and Data Loading\n",
    "\n",
    "Let's start by importing the necessary libraries for text processing and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "id": "afc8f167-5477-48a0-8e2a-6abaa421c89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:44:56.718959Z",
     "start_time": "2025-11-28T10:44:55.579462Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.pyplot import plot, show\n",
    "\n",
    "RANDOM_STATE = 3\n",
    "TRAIN_TEST_SPLIT_SIZE = 0.2"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "cf30c64f-75c7-48b0-9e74-c0e481c190f4",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Read the input data and check its sanity\n",
    "We have two annotated corpora:\n",
    "* SMS messages and their classes;\n",
    "* email messages and their classes.\n",
    "\n",
    "Understanding the characteristics and quality of our datasets is essential before building models. We need to check for duplicate entries, data imbalance, and basic statistics to ensure robust model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5c9f4",
   "metadata": {},
   "source": [
    "## 2.1 Initial Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bc82ade-150e-46d0-9787-4d6f219b0359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:47:25.029569Z",
     "start_time": "2025-11-28T10:47:24.880769Z"
    }
   },
   "source": [
    "# Load the datasets\n",
    "sms_data = pd.read_csv('../data/sms_spam.csv',sep=';')\n",
    "email_data = pd.read_csv('../data/email_spam.csv')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:49:29.930362Z",
     "start_time": "2025-11-28T10:49:29.912754Z"
    }
   },
   "cell_type": "code",
   "source": "email_data",
   "id": "aafa17c98d753df8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 sender  \\\n",
       "0                        Robert Elz <kre@munnari.OZ.AU>   \n",
       "1             Steve Burt <Steve_Burt@cursor-system.com>   \n",
       "2                         \"Tim Chapman\" <timc@2ubh.com>   \n",
       "3                      Monty Solomon <monty@roscom.com>   \n",
       "4             Stewart Smith <Stewart.Smith@ee.ed.ac.uk>   \n",
       "...                                                 ...   \n",
       "5804  Professional_Career_Development_Institute@Frug...   \n",
       "5805                          \"IQ - TBA\" <tba@insiq.us>   \n",
       "5806                               Mike <raye@yahoo.lv>   \n",
       "5807                   \"Mr. Clean\" <cweqx@dialix.oz.au>   \n",
       "5808  \"wilsonkamela400@netscape.net\" <wilsonkamela50...   \n",
       "\n",
       "                                               receiver  \\\n",
       "0     Chris Garrigues <cwg-dated-1030377287.06fa6d@D...   \n",
       "1     \"'zzzzteana@yahoogroups.com'\" <zzzzteana@yahoo...   \n",
       "2                 zzzzteana <zzzzteana@yahoogroups.com>   \n",
       "3                              undisclosed-recipient: ;   \n",
       "4                             zzzzteana@yahoogroups.com   \n",
       "...                                                 ...   \n",
       "5804                                yyyy@netnoteinc.com   \n",
       "5805                      <yyyy@spamassassin.taint.org>   \n",
       "5806                      Mailing.List@user2.pro-ns.net   \n",
       "5807               <Undisclosed.Recipients@webnote.net>   \n",
       "5808                                      ilug@linux.ie   \n",
       "\n",
       "                                 date  \\\n",
       "0     Thu, 22 Aug 2002 18:26:25 +0700   \n",
       "1     Thu, 22 Aug 2002 12:46:18 +0100   \n",
       "2     Thu, 22 Aug 2002 13:52:38 +0100   \n",
       "3     Thu, 22 Aug 2002 09:15:25 -0400   \n",
       "4     Thu, 22 Aug 2002 14:38:22 +0100   \n",
       "...                               ...   \n",
       "5804   Tue, 3 Dec 2002 13:19:58 -0800   \n",
       "5805   Tue, 3 Dec 2002 18:52:29 -0500   \n",
       "5806  Sun, 20 Jul 2003 16:19:44 +0800   \n",
       "5807  Wed, 05 Aug 2020 04:01:50 -1900   \n",
       "5808        Wed, 04 Dec 2002 06:07:07   \n",
       "\n",
       "                                                subject  \\\n",
       "0                              Re: New Sequences Window   \n",
       "1                             [zzzzteana] RE: Alexander   \n",
       "2                             [zzzzteana] Moscow bomber   \n",
       "3                 [IRR] Klez: The Virus That  Won't Die   \n",
       "4        Re: [zzzzteana] Nothing like mama used to make   \n",
       "...                                                 ...   \n",
       "5804                      Busy? Home Study Makes Sense!   \n",
       "5805             Preferred Non-Smoker Rates for Smokers   \n",
       "5806  How to get 10,000 FREE hits per day to any web...   \n",
       "5807                                Cannabis Difference   \n",
       "5808                              [ILUG] WILSON  KAMELA   \n",
       "\n",
       "                                                message  label  urls  \n",
       "0     Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...      0     1  \n",
       "1     Martin A posted:\\nTassos Papadopoulos, the Gre...      0     1  \n",
       "2     Man Threatens Explosion In Moscow \\n\\nThursday...      0     1  \n",
       "3     Klez: The Virus That Won't Die\\n \\nAlready the...      0     1  \n",
       "4     >  in adding cream to spaghetti carbonara, whi...      0     1  \n",
       "...                                                 ...    ...   ...  \n",
       "5804  \\n\\n  \\n---  \\n![](http://images.pcdi-homestud...      1     1  \n",
       "5805  This is a multi-part message in MIME format. -...      1     1  \n",
       "5806  Dear Subscriber,\\n\\nIf I could show you a way ...      1     1  \n",
       "5807  ****Mid-Summer Customer Appreciation SALE!****...      1     0  \n",
       "5808  ATTN:SIR/MADAN      \\n\\n                      ...      1     1  \n",
       "\n",
       "[5809 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert Elz &lt;kre@munnari.OZ.AU&gt;</td>\n",
       "      <td>Chris Garrigues &lt;cwg-dated-1030377287.06fa6d@D...</td>\n",
       "      <td>Thu, 22 Aug 2002 18:26:25 +0700</td>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve Burt &lt;Steve_Burt@cursor-system.com&gt;</td>\n",
       "      <td>\"'zzzzteana@yahoogroups.com'\" &lt;zzzzteana@yahoo...</td>\n",
       "      <td>Thu, 22 Aug 2002 12:46:18 +0100</td>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Tim Chapman\" &lt;timc@2ubh.com&gt;</td>\n",
       "      <td>zzzzteana &lt;zzzzteana@yahoogroups.com&gt;</td>\n",
       "      <td>Thu, 22 Aug 2002 13:52:38 +0100</td>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monty Solomon &lt;monty@roscom.com&gt;</td>\n",
       "      <td>undisclosed-recipient: ;</td>\n",
       "      <td>Thu, 22 Aug 2002 09:15:25 -0400</td>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stewart Smith &lt;Stewart.Smith@ee.ed.ac.uk&gt;</td>\n",
       "      <td>zzzzteana@yahoogroups.com</td>\n",
       "      <td>Thu, 22 Aug 2002 14:38:22 +0100</td>\n",
       "      <td>Re: [zzzzteana] Nothing like mama used to make</td>\n",
       "      <td>&gt;  in adding cream to spaghetti carbonara, whi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>Professional_Career_Development_Institute@Frug...</td>\n",
       "      <td>yyyy@netnoteinc.com</td>\n",
       "      <td>Tue, 3 Dec 2002 13:19:58 -0800</td>\n",
       "      <td>Busy? Home Study Makes Sense!</td>\n",
       "      <td>\\n\\n  \\n---  \\n![](http://images.pcdi-homestud...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>\"IQ - TBA\" &lt;tba@insiq.us&gt;</td>\n",
       "      <td>&lt;yyyy@spamassassin.taint.org&gt;</td>\n",
       "      <td>Tue, 3 Dec 2002 18:52:29 -0500</td>\n",
       "      <td>Preferred Non-Smoker Rates for Smokers</td>\n",
       "      <td>This is a multi-part message in MIME format. -...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>Mike &lt;raye@yahoo.lv&gt;</td>\n",
       "      <td>Mailing.List@user2.pro-ns.net</td>\n",
       "      <td>Sun, 20 Jul 2003 16:19:44 +0800</td>\n",
       "      <td>How to get 10,000 FREE hits per day to any web...</td>\n",
       "      <td>Dear Subscriber,\\n\\nIf I could show you a way ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>\"Mr. Clean\" &lt;cweqx@dialix.oz.au&gt;</td>\n",
       "      <td>&lt;Undisclosed.Recipients@webnote.net&gt;</td>\n",
       "      <td>Wed, 05 Aug 2020 04:01:50 -1900</td>\n",
       "      <td>Cannabis Difference</td>\n",
       "      <td>****Mid-Summer Customer Appreciation SALE!****...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>\"wilsonkamela400@netscape.net\" &lt;wilsonkamela50...</td>\n",
       "      <td>ilug@linux.ie</td>\n",
       "      <td>Wed, 04 Dec 2002 06:07:07</td>\n",
       "      <td>[ILUG] WILSON  KAMELA</td>\n",
       "      <td>ATTN:SIR/MADAN      \\n\\n                      ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5809 rows Ã— 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1db7a3ca",
   "metadata": {},
   "source": [
    "**Exercise:** Check for duplicate entries and data quality issues\n",
    "\n",
    "Duplicate entries can artificially inflate performance if the same message appears in both training and test sets. Removing duplicates ensures fair evaluation and prevents data leakage.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.drop_duplicates()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)\n",
    "- [Data quality assessment](https://pandas.pydata.org/docs/user_guide/duplicates.html)\n",
    "\n",
    "Use the `drop_duplicates()` method to remove any duplicate entries from both datasets. Set `inplace=True` and `ignore_index=True` to modify the datasets directly and reset row indices."
   ]
  },
  {
   "cell_type": "code",
   "id": "9edc05c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:50:41.073990Z",
     "start_time": "2025-11-28T10:50:40.993472Z"
    }
   },
   "source": [
    "# Check for the existence of duplicate entries and eliminate them if necessary.\n",
    "\n",
    "email_data.drop_duplicates(inplace=True, ignore_index=True)\n",
    "sms_data.drop_duplicates(inplace=True, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "af31a508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:50:48.040631Z",
     "start_time": "2025-11-28T10:50:48.034701Z"
    }
   },
   "source": [
    "# Extract messages and labels for easier handling\n",
    "sms_messages = sms_data[\"message\"]\n",
    "sms_labels = sms_data[\"label\"]\n",
    "email_messages = email_data[\"message\"]\n",
    "email_labels = email_data[\"label\"]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "7717e198",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Splitting Strategies\n",
    "\n",
    "Different experimental scenarios require different data splitting approaches. We'll implement four strategies to explore various aspects of text classification performance:\n",
    "\n",
    "1. **Train/Test on SMS**: Standard evaluation within SMS domain\n",
    "2. **Train/Test on Email**: Standard evaluation within email domain  \n",
    "3. **Transfer Learning**: Train on SMS, test on email (domain adaptation)\n",
    "4. **Combined Training**: Train and test on merged datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab9253",
   "metadata": {},
   "source": [
    "**Exercise:** Implement SMS dataset splitting function\n",
    "\n",
    "Standard train-test splitting allows us to evaluate model performance within the SMS domain. This provides a baseline for comparison with other scenarios.\n",
    "\n",
    "**Documentation references:**\n",
    "- [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [Random state for reproducibility](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "id": "13940249-b044-40d8-87c6-e1af46a82c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:57:54.921109Z",
     "start_time": "2025-11-28T10:57:54.910541Z"
    }
   },
   "source": [
    "def train_eval_sms():\n",
    "    \"\"\"\n",
    "    Split SMS dataset into training and testing sets.\n",
    "    \n",
    "    Creates a standard train-test split for SMS spam detection evaluation.\n",
    "    Uses stratified sampling to maintain class distribution across splits.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing (train_messages, test_messages, train_labels, test_labels)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Uses global TRAIN_TEST_SPLIT_SIZE and RANDOM_STATE for consistency\n",
    "    across all experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    return train_test_split(sms_messages, sms_labels, test_size=TRAIN_TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=sms_labels)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "cbd08776",
   "metadata": {},
   "source": [
    "**Exercise:** Implement email dataset splitting function\n",
    "\n",
    "Similar to SMS splitting, this function enables evaluation within the email domain to establish baseline performance for email spam detection."
   ]
  },
  {
   "cell_type": "code",
   "id": "56127f57-c39f-4402-8a96-3dd91c1a960d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T10:57:56.658233Z",
     "start_time": "2025-11-28T10:57:56.655647Z"
    }
   },
   "source": [
    "def train_eval_email():\n",
    "    \"\"\"\n",
    "    Split email dataset into training and testing sets.\n",
    "    \n",
    "    Creates a standard train-test split for email spam detection evaluation.\n",
    "    Maintains consistent splitting parameters with SMS evaluation for fair comparison.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing (train_messages, test_messages, train_labels, test_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    return train_test_split(email_messages, email_labels, test_size=TRAIN_TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=email_labels)\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "cdbb0268",
   "metadata": {},
   "source": [
    "**Exercise:** Implement cross-domain transfer function\n",
    "\n",
    "Domain transfer testing reveals how well models trained on one type of text (SMS) perform on another (email). This scenario is common in real-world applications where training data and deployment contexts differ."
   ]
  },
  {
   "cell_type": "code",
   "id": "6d969741-7ddf-4f0b-ad7b-8b2a71aff351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T11:00:10.201474Z",
     "start_time": "2025-11-28T11:00:10.195078Z"
    }
   },
   "source": [
    "def train_sms_eval_email():\n",
    "    \"\"\"\n",
    "    Prepare data for cross-domain transfer learning experiment.\n",
    "    \n",
    "    Uses entire SMS dataset for training and entire email dataset for testing.\n",
    "    This setup evaluates model generalization across different text domains\n",
    "    and communication channels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing (train_messages, test_messages, train_labels, test_labels)\n",
    "        where training data comes from SMS and testing data from email\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    No random splitting is performed as we use complete datasets for \n",
    "    cross-domain evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    return sms_messages, email_messages, sms_labels, email_labels"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "883bead5",
   "metadata": {},
   "source": [
    "**Exercise:** Implement combined dataset function\n",
    "\n",
    "Training on combined data tests whether mixing domains improves overall performance and provides insights into dataset complementarity for spam detection.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.concat() for combining datasets](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "9de8781a-628f-4321-bdf7-c8887f07dd2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T11:04:17.117922Z",
     "start_time": "2025-11-28T11:04:17.108248Z"
    }
   },
   "source": [
    "def train_eval_combined():\n",
    "    \"\"\"\n",
    "    Combine SMS and email datasets for unified training and testing.\n",
    "    \n",
    "    Merges both datasets and creates a mixed train-test split. This approach\n",
    "    evaluates whether combining different text domains improves overall\n",
    "    spam detection performance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing (train_messages, test_messages, train_labels, test_labels)\n",
    "        from the combined dataset\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Uses pandas.concat to merge datasets while preserving all data points.\n",
    "    Maintains class balance across the combined dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = pd.concat([sms_messages, email_messages], axis=0)\n",
    "    labels = pd.concat([sms_labels, email_labels], axis=0)\n",
    "    \n",
    "    return train_test_split(messages, labels, test_size=TRAIN_TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=labels)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "5178db71-e7a9-46cb-a068-66d7d11ada72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T11:04:19.495897Z",
     "start_time": "2025-11-28T11:04:19.449669Z"
    }
   },
   "source": [
    "# Initialize with combined dataset for demonstration\n",
    "training_messages, testing_messages, training_labels, testing_labels = train_eval_combined()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "65393c14-4cb2-4525-9a1f-5aaddb126b62",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Data Balancing and Class Distribution\n",
    "\n",
    "Class imbalance is a common problem in spam detection where spam messages are typically much less frequent than legitimate messages. Imbalanced datasets can lead to biased models that perform poorly on minority classes. We need to address this issue to ensure fair evaluation and robust model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37f460",
   "metadata": {},
   "source": [
    "## 3.1 Class Imbalance Detection and Correction\n",
    "\n",
    "Balanced training data ensures that models learn both spam and non-spam patterns equally well. Oversampling the minority class is a simple and effective approach for text classification.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.sample()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)\n",
    "- [Class imbalance handling techniques](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "110b2568-f3a6-440e-96d9-adc9d398f7ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T11:04:51.038377Z",
     "start_time": "2025-11-28T11:04:51.027966Z"
    }
   },
   "source": [
    "def balance(training_messages, training_labels):\n",
    "    \"\"\"\n",
    "    Balance training data by oversampling the minority class.\n",
    "    \n",
    "    Addresses class imbalance by randomly sampling additional instances\n",
    "    from the underrepresented class until both classes have equal frequency.\n",
    "    This prevents model bias toward the majority class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_messages : pandas.Series\n",
    "        Training text messages\n",
    "    training_labels : pandas.Series  \n",
    "        Corresponding class labels (0 for ham, 1 for spam)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing (balanced_messages, balanced_labels) with equal\n",
    "        class representation\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Uses random sampling with replacement to increase minority class size.\n",
    "    Preserves original data distribution while achieving balance.\n",
    "    \"\"\"\n",
    "    print(\"Label counts before balancing:\")\n",
    "    print(training_labels.value_counts())\n",
    "    \n",
    "    counts = training_labels.value_counts()\n",
    "    if counts[1] > counts[0]:\n",
    "        label_to_oversample = 0\n",
    "        diff = counts[1] - counts[0]\n",
    "    else:\n",
    "        label_to_oversample = 1\n",
    "        diff = counts[0] - counts[1]\n",
    "    \n",
    "    training_data = pd.concat([training_messages, training_labels], axis=1)\n",
    "    draw_from = training_data[training_data[\"label\"] == label_to_oversample]\n",
    "    \n",
    "    for i in range(diff):\n",
    "        sample = draw_from.sample(random_state=RANDOM_STATE)\n",
    "        training_data = pd.concat([training_data, sample])\n",
    "    \n",
    "    training_messages = training_data[\"message\"]\n",
    "    training_labels = training_data[\"label\"]\n",
    "    \n",
    "    print(\"Label counts after balancing:\")\n",
    "    print(training_labels.value_counts())\n",
    "    return training_messages, training_labels"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "5c7ffd32",
   "metadata": {},
   "source": [
    "**Exercise:** Apply balancing to training data\n",
    "\n",
    "Check if the current training data is balanced and apply correction if needed. Balanced training data is crucial for fair model evaluation and optimal performance on both classes."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f32b2a8-03ea-415a-9581-a77dc9487f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T11:07:12.617805Z",
     "start_time": "2025-11-28T11:07:10.993356Z"
    }
   },
   "source": [
    "#Check if our training data is balanced and apply balancing if necessary\n",
    "#training_labels.value_counts()\n",
    "\n",
    "# 6885 versus 1896 -> needs to be balanced\n",
    "training_messages, training_labels = balance(training_messages, training_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts before balancing:\n",
      "label\n",
      "0    6885\n",
      "1    1896\n",
      "Name: count, dtype: int64\n",
      "Label counts after balancing:\n",
      "label\n",
      "0    6885\n",
      "1    6885\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "287b2416-cadd-40ea-a748-b744e88f0386",
   "metadata": {},
   "source": [
    "***\n",
    "# 4. Text Preprocessing and Feature Extraction\n",
    "\n",
    "## 4.1 Understanding Tokenization Strategies\n",
    "\n",
    "We will take the simplest approach possible: our input features will be the most frequent words and their frequencies. The idea is that the words that appear frequently in a document are characteristic of its content, and thus of its spam-ness.\n",
    "\n",
    "The **CountVectorizer** class of scikit-learn will do exactly this for us. It first counts word frequencies across *all* messages, in order to find the overall most frequent ones. Then, it counts the occurrences of these most frequent words in each message, computing a frequency vector per message, where each dimension of the vector corresponds to a frequent word.\n",
    "\n",
    "Firstly, what does *most frequent word* mean? We will define a threshold, which we will call the **number of features**.\n",
    "\n",
    "Secondly, what is a word? *Word* is not a term from linguistics, it has no scientific definition.\n",
    "* Is \"hazelnuts\" one word, two words, or three words? \"hazel\", \"nut\", and \"-s\" are what linguists call *morphemes*: the elementary units of meaning, but in common language \"hazelnuts\" would be considered as a single word.\n",
    "* Is \"$12.50\" a single word? It consists of a currency symbol and a rational number.\n",
    "* Is \"Joe's\" one or two words?\n",
    "* etc.\n",
    "\n",
    "A pragmatic choice is not to use the term \"word\" but rather the term \"token\". A token can be whatever unit into which we decide to split our input text. We call **tokenization** the process of splitting a text into tokens. **Beware: the choice of splitting rule will determine the performance of downstream tasks.** CountVectorizer has a **token_pattern** parameter that takes a regular expression as an input string. Instead of blindly trusting whatever default tokenization method offered by CountVectorizer, let us define our own rule. A few possibilities:\n",
    "* split by whitespace;\n",
    "* split by whitespace or punctuation;\n",
    "* keep only tokens that contain letters or digits;\n",
    "* keep only tokens of length > X (where you choose X);\n",
    "* etc.\n",
    "\n",
    "Furthermore, it is common to perform additional preprocessing to the input text, always depending on the requirements of the downstream task:\n",
    "* in some cases, converting to all-lowercase may improve results (e.g. \"WIN\" and \"win\" are collapsed into a single feature), but it may also result in losing useful information (is all-caps characteristic of spam?).\n",
    "* In bag-of-word models, removing so-called *stop words* helps eliminate frequent grammatical words that bear little relevant meaning (e.g. articles, pronouns, modal verbs, prepositions). Beware, as models that rely on syntax (i.e. phrases) do need grammar words: stop words should not be eliminated systematically.\n",
    "* The presence of numbers (e.g. phone numbers, money amounts) in the text may be an important feature to detect spam. However, each distinct number will be considered by CountVectorizer as a different token, and as phone numbers (sums of money, etc.) tend not to repeat across messages, the classifier will not be able to generalize over them. You may implement generalisation manually by detecting tokens that only contain digits and replacing them by a general \"<NUMBER>\" token, for example.     \n",
    "\n",
    "CountVectorizer is a powerful tool that has built-in support for both lowercase conversion and removal of English stop words, so you do not need to implement these preprocessing operations by hand. \n",
    "\n",
    "**Documentation references:**\n",
    "- [Text feature extraction guide](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "- [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [Regular expressions for tokenization and for finding numbers](https://docs.python.org/3/library/re.html#regular-expression-syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70550aa-82e6-42dc-a6f8-caf6666bb854",
   "metadata": {},
   "source": [
    "**Preprocess numbers appearing in messages**\n",
    "\n",
    "Replace numbers such as \"5000\" or \"0612345678\" by a general \"<NUM>\" token, helping the classifier generalise over such tokens. You can use the `re.sub` method to find patterns and replace them in strings. "
   ]
  },
  {
   "cell_type": "code",
   "id": "836a12d8-7207-4d9c-9e17-684545d76bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:29:47.030527Z",
     "start_time": "2025-11-28T14:29:47.008521Z"
    }
   },
   "source": [
    "# Implement a preprocessing method that replaces numbers by a generic placeholder, and possibly other simplifications\n",
    "def preprocess(txt):\n",
    "    \"\"\"\n",
    "    Replace all numbers in text with <NUM> token.\n",
    "\n",
    "    Args:\n",
    "        txt (str): Input text containing numbers\n",
    "\n",
    "    Returns:\n",
    "        str: Text with numbers replaced by <NUM>\n",
    "    \"\"\"\n",
    "    \n",
    "    return re.sub(r'\\d+', '<NUM>', txt)\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "f7930247",
   "metadata": {},
   "source": [
    "**Configure tokenization and feature extraction parameters**\n",
    "\n",
    "The `token_pattern` parameter controls how text is split into tokens. Different patterns can significantly impact model performance by determining which linguistic units are considered as features.\n",
    "\n",
    "**Exercise:** experiment with different regular expressions for tokenization and  with different numbers of features. For example, the regex `\"([A-Za-z0-9][A-Za-z0-9]+)\"` will only extract tokens that are at least two characters long and that contain only numbers or letters between a and z."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d0e8330-b878-41a8-b49f-64bfa3d4b75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:29:48.718448Z",
     "start_time": "2025-11-28T14:29:48.713292Z"
    }
   },
   "source": [
    "# Define the regular expression that extracts tokens.\n",
    "# Within the regex, there should be exactly one parenthesised expression\n",
    "# that will capture the token to be extracted.\n",
    "\n",
    "# The following example extracts series of non-whitespace characters.\n",
    "TOKEN_REGEX = r\"(\\S+)\"\n",
    "NB_FEATURES = 5000"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "6b09c3ef",
   "metadata": {},
   "source": [
    "**Exercise:** Initialize text vectorization with CountVectorizer\n",
    "\n",
    "CountVectorizer converts text documents into numerical feature vectors by counting token occurrences. It first builds a vocabulary from the most frequent tokens, then represents each document as a vector of token counts.\n",
    "\n",
    "**Key parameters:**\n",
    "- `max_features`: Limits vocabulary size to most frequent tokens\n",
    "- `token_pattern`: Regular expression defining what constitutes a token\n",
    "- `lowercase`: Whether to convert text to lowercase before tokenization\n",
    "- `stop_words`: Whether to remove common English stop words\n",
    "- `preprocessor`: calls your custom text preprocessor function given as argument (warning: it overrides the `lowercase` setting!)\n",
    "\n",
    "Initialize CountVectorizer with the defined parameters to prepare for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "id": "811a8d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:38:04.066026Z",
     "start_time": "2025-11-28T14:38:04.055688Z"
    }
   },
   "source": [
    "# Call CountVectorizer with the parameters you wish to use\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    input='content',\n",
    "    max_features=NB_FEATURES,\n",
    "    token_pattern=TOKEN_REGEX,\n",
    "    preprocessor=preprocess,\n",
    "    #stop_words=[]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "12f8169d",
   "metadata": {},
   "source": [
    "## 4.2 Feature Matrix Creation\n",
    "\n",
    "The vectorization process has two phases:\n",
    "- **Fit**: Analyzes training text to build vocabulary of most frequent tokens\n",
    "- **Transform**: Converts text documents into numerical feature vectors using the learned vocabulary\n",
    "\n",
    "**Documentation references:**\n",
    "- [Fit vs Transform in scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "- [Feature extraction workflow](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f70e7",
   "metadata": {},
   "source": [
    "**Exercise:** Create feature matrices for training and testing\n",
    "\n",
    "Use `fit_transform()` on training data to learn vocabulary and create features simultaneously:\n",
    "- \"fit\" computes the *vocabulary* consisting of the most frequent tokens.\n",
    "- \"transform\" computes the *frequencies* of tokens in the vocabulary, which will be our input features. \n",
    "\n",
    "Use `transform()` on testing data to convert it using the same vocabulary learned from training, ensuring consistency between training and testing representations.\n",
    "\n",
    "You can use `vectorizer.get_feature_names_out()` to obtain the list of features (= most frequent tokens) extracted."
   ]
  },
  {
   "cell_type": "code",
   "id": "8da1c0f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:38:37.861357Z",
     "start_time": "2025-11-28T14:38:37.003039Z"
    }
   },
   "source": [
    "# Fit and transform with the vectorizer on the training messages\n",
    "\n",
    "train_vectorized = count_vectorizer.fit_transform(training_messages)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "54c7da5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:38:38.784351Z",
     "start_time": "2025-11-28T14:38:38.629852Z"
    }
   },
   "source": [
    "# Do transform with the vectorizer on the testing messages\n",
    "\n",
    "test_vectorized = count_vectorizer.transform(testing_messages)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "53de2265-bc54-4224-9c79-f6cc226432ba",
   "metadata": {},
   "source": [
    "***\n",
    "# 5. Model Training with Logistic Regression\n",
    "\n",
    "We will use one of the simplest and fastest machine learning models that exist: a **logistic regression classifier**.\n",
    "\n",
    "Logistic regression is a binary classifier, which suits our task well. The only input hyperparameter we will use is the number of iterations.\n",
    "\n",
    "We could also use other classifiers, such as an SVM, but the goal of this lab is to get familiar with a few fundamental notions of natural language processing, not to find the best machine learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec15717",
   "metadata": {},
   "source": [
    "## 5.1 Model Configuration and Training\n",
    "\n",
    "**Documentation references:**\n",
    "- [Logistic Regression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Text classification with scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "75dbd6df-fe69-4ce0-b235-c3be76aefb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:46:19.417003Z",
     "start_time": "2025-11-28T14:46:19.415059Z"
    }
   },
   "source": [
    "# Model hyperparameters\n",
    "NB_ITERATIONS = 1000"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "dfd17e26",
   "metadata": {},
   "source": [
    "**Exercise:** Initialize and train the logistic regression model\n",
    "\n",
    "Configure Logistic Regression with sufficient iterations to ensure convergence on high-dimensional text features. Train the model on the preprocessed feature matrix to learn patterns distinguishing spam from legitimate messages."
   ]
  },
  {
   "cell_type": "code",
   "id": "a30bc582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:46:20.338580Z",
     "start_time": "2025-11-28T14:46:20.327483Z"
    }
   },
   "source": [
    "# Instantiate a logistic regression model with the number of iterations as an input hyperparameter\n",
    "\n",
    "logistic_reg = LogisticRegression(\n",
    "    penalty='l2', # Can be changed\n",
    "    max_iter=NB_ITERATIONS\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "d71edb22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:47:44.498174Z",
     "start_time": "2025-11-28T14:47:43.950380Z"
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "model = logistic_reg.fit(train_vectorized, training_labels)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "b612598a-c191-409b-a193-98eedda7cde7",
   "metadata": {},
   "source": [
    "***\n",
    "# 6. Model Evaluation and Metrics\n",
    "\n",
    "Evaluation metrics provide different perspectives on model performance. For spam detection, we need to understand not just overall accuracy but also how well the model identifies spam (precision) and how many spam messages it catches (recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164e9b6-2e23-466d-a90a-c2c37dc29406",
   "metadata": {},
   "source": [
    "**Confusion Matrix Concepts:**\n",
    "\n",
    "|                | Predicted Ham | Predicted Spam |\n",
    "|----------------|:------------:|:-------------:|\n",
    "| **Actual Ham** | TN           | FP            |\n",
    "| **Actual Spam**| FN           | TP            |\n",
    "\n",
    "Where:\n",
    "- **True Positives (TP)**: Correctly identified spam\n",
    "- **True Negatives (TN)**: Correctly identified ham  \n",
    "- **False Positives (FP)**: Ham incorrectly labeled as spam\n",
    "- **False Negatives (FN)**: Spam incorrectly labeled as ham\n",
    "\n",
    "**Documentation references:**\n",
    "- [Classification metrics guide](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "- [Confusion matrix interpretation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bb110-f3f3-4393-a17d-fad71b21d336",
   "metadata": {},
   "source": [
    "### 6.1.1 Accuracy Implementation\n",
    "\n",
    "**Exercise:** Implement accuracy calculation from scratch\n",
    "\n",
    "Accuracy measures the proportion of correct predictions (both spam and ham) out of all predictions. While intuitive, accuracy can be misleading with imbalanced datasets where a model could achieve high accuracy by always predicting the majority class.\n",
    "\n",
    "**Formula:** Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd934b23-f400-4d83-910e-613893baa830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:07:56.771750Z",
     "start_time": "2025-11-28T15:07:56.764676Z"
    }
   },
   "source": [
    "# Compute accuracy by comparing truth and predicted labels\n",
    "def accuracy_score(truth, pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy as the proportion of correct predictions.\n",
    "    \n",
    "    Accuracy measures overall correctness but may not reflect performance\n",
    "    on individual classes, especially with imbalanced datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    truth : array-like\n",
    "        Ground truth labels (0 for ham, 1 for spam)\n",
    "    pred : array-like  \n",
    "        Predicted labels from the model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy score between 0 and 1, where 1 indicates perfect accuracy\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Accuracy alone may be misleading for imbalanced datasets where\n",
    "    a model could achieve high accuracy by always predicting the majority class.\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(pred)\n",
    "    \n",
    "    return sum([truth.iloc[i] == pred[i] for i in range(len(truth))]) / len(truth)\n"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "78ef238a-4513-4957-b39c-af2758a4cba3",
   "metadata": {},
   "source": [
    "### 6.1.2 Precision Implementation\n",
    "\n",
    "**Exercise:** Implement precision calculation from scratch\n",
    "\n",
    "Precision measures the proportion of predicted spam that is actually spam. High precision means few false alarms (legitimate messages incorrectly flagged as spam), which is crucial for user experience.\n",
    "\n",
    "**Formula:** Precision = TP / (TP + FP)\n",
    "\n",
    "**Documentation references:**\n",
    "- [Precision definition](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c071a9a1-926b-4804-b850-1b23df8115fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:08:25.602647Z",
     "start_time": "2025-11-28T15:08:25.589482Z"
    }
   },
   "source": [
    "# Compute precision by calculating true positives and false positives\n",
    "def precision_score(truth, pred, pos_label):\n",
    "    \"\"\"\n",
    "    Calculate precision for the positive class (spam).\n",
    "    \n",
    "    Precision measures the proportion of predicted spam messages that are\n",
    "    actually spam. High precision indicates few false positive errors\n",
    "    (legitimate messages incorrectly classified as spam).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    truth : array-like\n",
    "        Ground truth labels\n",
    "    pred : array-like\n",
    "        Predicted labels from the model  \n",
    "    pos_label : int or str\n",
    "        Label that represents the positive class (spam)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Precision score between 0 and 1, where 1 indicates perfect precision\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Precision is especially important in spam detection to minimize\n",
    "    false positives that could cause users to miss important messages.\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(pred) \n",
    "    \n",
    "    TP = 0\n",
    "    TP_FP = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth.iloc[i] == pred[i] == pos_label:\n",
    "            TP += 1\n",
    "        if pred[i] == pos_label:\n",
    "            TP_FP += 1\n",
    "    \n",
    "    return TP / TP_FP\n",
    "    \n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "5b8a92b4-73f8-4506-b436-65ab371c7af0",
   "metadata": {},
   "source": [
    "### 6.1.3 Recall Implementation\n",
    "\n",
    "**Exercise:** Implement recall calculation from scratch\n",
    "\n",
    "Recall measures the proportion of actual spam that the model correctly identifies. High recall means the model catches most spam messages, which is important for protecting users from unwanted content.\n",
    "\n",
    "**Formula:** Recall = TP / (TP + FN)\n",
    "\n",
    "**Documentation references:**\n",
    "- [Recall definition](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bef05c6-3911-41c6-a943-81c92aa1d2c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:08:00.405511Z",
     "start_time": "2025-11-28T15:08:00.395312Z"
    }
   },
   "source": [
    "# Compute recall by calculating true positives and false negatives\n",
    "def recall_score(truth, pred, pos_label):\n",
    "    \"\"\"\n",
    "    Calculate recall for the positive class (spam).\n",
    "    \n",
    "    Recall measures the proportion of actual spam messages that the model\n",
    "    correctly identifies. High recall indicates the model catches most\n",
    "    spam with few false negative errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    truth : array-like\n",
    "        Ground truth labels\n",
    "    pred : array-like\n",
    "        Predicted labels from the model\n",
    "    pos_label : int or str  \n",
    "        Label that represents the positive class (spam)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Recall score between 0 and 1, where 1 indicates perfect recall\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Recall is crucial in spam detection to ensure most unwanted messages\n",
    "    are filtered out, protecting users from spam content.\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(pred) \n",
    "    \n",
    "    TP = 0\n",
    "    TP_FN = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth.iloc[i] == pred[i] == pos_label:\n",
    "            TP += 1\n",
    "            TP_FN += 1\n",
    "        if (pred[i] != pos_label) and (truth.iloc[i] != pred[i]):\n",
    "            TP_FN += 1\n",
    "    \n",
    "    return TP / TP_FN\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "dc39a571-205b-4e4c-a707-5411ffb9e6de",
   "metadata": {},
   "source": [
    "## 6.2 Model Prediction and Performance Analysis\n",
    "\n",
    "**Exercise:** Generate predictions and calculate comprehensive metrics\n",
    "\n",
    "Use the trained model to make predictions on the test set, then calculate all three metrics to get a complete picture of model performance. Compare these metrics to understand the trade-offs between accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "id": "5af8cfc4-34a6-440d-9681-7931132e66da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:05:52.344264Z",
     "start_time": "2025-11-28T15:05:52.326134Z"
    }
   },
   "source": [
    "# Generate predictions using the trained model\n",
    "\n",
    "pred = model.predict(test_vectorized)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:07:17.020952Z",
     "start_time": "2025-11-28T15:07:17.007404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "testing_labels.iloc[0]"
   ],
   "id": "7b660c4de213fe25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "43661709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:08:29.616439Z",
     "start_time": "2025-11-28T15:08:29.592227Z"
    }
   },
   "source": [
    "# Calculate all performance metrics\n",
    "acc = accuracy_score(testing_labels, pred)\n",
    "prec = precision_score(testing_labels, pred, 1)\n",
    "rec = recall_score(testing_labels, pred, 1)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "39158736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:08:31.183781Z",
     "start_time": "2025-11-28T15:08:31.181005Z"
    }
   },
   "source": [
    "print(\"Accuracy : \" + str(acc))\n",
    "print(\"Precision: \" + str(prec))\n",
    "print(\"Recall   : \" + str(rec))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9667577413479053\n",
      "Precision: 0.9445676274944568\n",
      "Recall   : 0.8987341772151899\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "841ff4b9",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "# 7. Experimental Scenarios and Comparative Analysis\n",
    "\n",
    "The following exercises guide you through different experimental scenarios to understand how text classification models perform across domains and datasets. Each scenario reveals different aspects of model generalization and domain adaptation.\n",
    "\n",
    "\n",
    "## 7.1 Single-Domain Experiments\n",
    "\n",
    "**Exercise:** Run experiments on SMS dataset only\n",
    "\n",
    "Modify the data loading section to use `train_eval_sms()` instead of the combined dataset. Observe how the model performs when trained and tested on the same text domain (SMS messages).\n",
    "\n",
    "**Questions to consider:**\n",
    "- How does performance compare to the combined dataset results?\n",
    "- Which metrics show the most significant changes?\n",
    "- What might explain any performance differences?\n",
    "\n",
    "\n",
    "**Exercise:** Run experiments on email dataset only\n",
    "\n",
    "Switch to using `train_eval_email()` to train and test exclusively on email data. Compare results with the SMS-only experiment.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Do emails and SMS messages show similar classification difficulty?\n",
    "- Which dataset appears more challenging for spam detection?\n",
    "- How do the optimal features differ between domains?\n",
    "\n",
    "\n",
    "## 7.2 Cross-Domain Transfer Learning\n",
    "\n",
    "**Exercise:** Train on SMS, evaluate on email dataset\n",
    "\n",
    "Use `train_sms_eval_email()` to explore domain transfer performance. This simulates a realistic scenario where you have labeled data from one domain but need to deploy in another.\n",
    "\n",
    "**Questions to consider:**\n",
    "- How much does performance degrade when transferring across domains?\n",
    "- Which metrics are most affected by domain mismatch?\n",
    "- What linguistic differences between SMS and email might explain the results?\n",
    "\n",
    "\n",
    "## 7.3 Combined Dataset Analysis\n",
    "\n",
    "**Exercise:** Train and evaluate on combined datasets\n",
    "\n",
    "Return to using `train_eval_combined()` to assess whether mixing domains during training improves overall robustness.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Does combined training improve generalization across both domains?\n",
    "- How do results compare to single-domain experiments?\n",
    "- What are the trade-offs of mixed-domain training?\n",
    "\n",
    "## 7.4 Add README file\n",
    "\n",
    "**Exercise:** Create a professional README file documenting your spam detection analysis\n",
    "\n",
    "Based on your experimental results across all scenarios (SMS-only, email-only, cross-domain transfer, and combined datasets), create a comprehensive README.md file that summarizes your key findings, methodology, and performance comparisons. Include quantitative results, optimal preprocessing configurations, and practical deployment recommendations as you learned before.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
